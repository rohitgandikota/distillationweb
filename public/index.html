<!doctype html>
<html lang="en">
<head>
<title>Distilling Diversity and Control in Diffusion Models</title>
<meta name="viewport" content="width=device-width,initial-scale=1" />
<meta name="description" content="Improving diversity in distilled diffusion models through strategic use of base models and introducing DT-Visualization." />
<meta property="og:title" content="Distilling Diversity and Control in Diffusion Models" />
<meta property="og:description" content="Improving diversity in distilled diffusion models through strategic use of base models and introducing DT-Visualization." />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" /> 
<meta name="twitter:title" content="Distilling Diversity and Control in Diffusion Models" />
<meta name="twitter:description" content="Improving diversity in distilled diffusion models through strategic use of base models and introducing DT-Visualization." />
<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">

<link href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-rwoIResjU2yc3z8GV/NPeZWAv56rSmLldC3R/AZzGRnGxQQKnKkoFVhFQhNUwEyJ" crossorigin="anonymous">
<script src="https://code.jquery.com/jquery-3.2.1.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Noto+Sans+Math&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
<link href="style.css" rel="stylesheet">

<style>
.relatedthumb {
  float:left; width: 200px; margin: 3px 10px 7px 0;
}
.relatedblock {
  clear: both;
  display: inline-block;
}
.bold-sc {
  font-variant: small-caps;
  font-weight: bold;
}
.cite, .citegroup {
  margin-bottom: 8px;
}
:target {
  background-color: yellow;
}
</style>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-FD12LWN557"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date()); gtag('config', 'G-FD12LWN557');
</script>

</head>
<body class="nd-docs">
<div class="nd-pageheader">
 <div class="container">
 <h1 class="lead">
 Distilling
 <nobr class="widenobr">Diversity and Control</nobr>
 in 
 <nobr class="widenobr">Diffusion Models</nobr>
 </h1>
<address>
  <nobr><a href="https://rohitgandikota.github.io/" target="_blank">Rohit Gandikota</a><sup>1</sup>,</nobr>
  <nobr><a href="https://baulab.info/" target="_blank">David Bau</a><sup>1</sup></nobr>
 <br>
  <nobr><sup>1</sup><a href="https://khoury.northeastern.edu/" target="_blank">Northeastern University</a></nobr>
</address>
 </div>
</div><!-- end nd-pageheader -->

<div class="container">
<div class="row justify-content-center text-center">

<p>
<a href="https://arxiv.org/pdf/2503.XXXXX" class="d-inline-block p-3 align-top" target="_blank"><img height="100" width="78" src="images/paper-thumb.png" style="border:1px solid; margin: 0 38px;" alt="ArXiv Preprint thumbnail" data-nothumb=""><br>ArXiv<br>Preprint</a>
<a href="https://github.com/rohitgandikota/distillation" class="d-inline-block p-3 align-top" target="_blank"><img height="100" width="78" src="images/code-thumb.png" style="border:1px solid; margin: 0 38px;" alt="Github code thumbnail" data-nothumb=""><br>Source Code<br>Github</a>
</p>

<div class="card" style="max-width: 1020px;">
<div class="card-block">
<h3>Understanding the Diversity-Efficiency Tradeoff in Diffusion Models</h3>
<p>
Diffusion models face a fundamental challenge: faster generation through distillation leads to "mode collapse" - producing nearly identical images despite different random seeds. These accelerated models sacrifice creative diversity for speed, limiting their practical usefulness.
</p>
<p>
Our research explores what happens during those crucial early generation steps that so dramatically affects diversity. We've discovered surprising insights about when and how variety emerges during the diffusion process, challenging the assumed tradeoff between computational efficiency and creative expression.
</p>

</div><!--card-block-->
</div><!--card-->

</div><!--row-->
  
<div class="row">
<div class="col">
  
<figure class="center_image" style="margin-top: 30px">
  <center><img src="images/paper/intro.png" style="width:100%; max-width:800px"></center>
  <figcaption>Figure 1: <b>Diversity Distillation:</b> (a) A base diffusion model generates diverse images but requires many computational steps (9.22 seconds). (b) A distilled model is much faster (0.64 seconds) but produces similar-looking images across different seeds. (c) Our method restores diversity while maintaining speed by using the base model for only the crucial first step. <b>Control Distillation:</b> (d) Control methods like Concept Sliders can be transferred between model types, allowing fine-grained creative control in fast models.</figcaption>
</figure>

<h2>Understanding the Diversity Loss Problem</h2>
<p>
Diffusion models work by gradually removing noise from a random starting point, step by step, until a clear image emerges. This process can require dozens or even hundreds of sequential steps, making it computationally expensive. When researchers "distill" these models to work in fewer steps, they essentially compress this denoising process.
</p>
<p>
The acceleration comes with a notable cost: the distilled models begin producing images that look very similar to each other, even when starting from different random noise patterns. This lack of variation limits their creative usefulness, as generating multiple options from the same prompt often yields nearly identical results.
</p>
<p>
Think of it like this: the original diffusion model is like an artist who carefully considers many possible interpretations of a concept before settling on a final painting. The distilled model, in contrast, rushes to the same conclusion every time, skipping the creative exploration phase.
</p>

<h2>A Surprising Discovery: Preserved Internal Knowledge</h2>
<p>
Our first key finding was unexpected: despite producing less diverse outputs, distilled models still maintain the same internal understanding of concepts as their slower counterparts. We discovered this by testing whether control mechanisms (like sliders that adjust specific image attributes) trained on the slow models would work on the fast ones, and vice versa.
</p>
<p>
The results were clear: controls for adjusting features like "big eyes," "pixel art style," or specific character attributes worked seamlessly across model types without any retraining. This demonstrated that the distilled models hadn't lost their understanding of these concepts—they just weren't expressing the full range of possibilities in their outputs.
</p>

<figure class="center_image" style="margin-top: 30px">
  <center><img src="images/paper/transfer.png" style="width:100%; max-width:800px"></center>
  <figcaption>Figure 2: Control mechanisms like Concept Sliders, Custom Diffusion, and DreamBooth trained on base models can be directly applied to distilled models without any additional training. This demonstrates that distilled models preserve their understanding of concepts despite producing less diverse outputs.</figcaption>
</figure>

<h2>Introducing DT-Visualization: Seeing Inside the Black Box</h2>
<p>
To understand why diversity collapses despite preserved knowledge, we developed a new technique called DT-Visualization (Diffusion Target Visualization). This method reveals what the model "thinks" the final image will look like at any intermediate step in the generation process.
</p>
<p>
Think of it as taking a time machine to the middle of the creation process and asking: "Based on what you've done so far, what do you think the final image will be?" Our technique allows us to see these predictions without waiting for the full generation to complete.
</p>

<figure class="center_image" style="margin-top: 30px">
  <center><img src="images/paper/dt_equation.png" style="width:100%; max-width:400px"></center>
  <figcaption>Figure 3: DT-Visualization lets us see what the model predicts as the final output at each intermediate step. This helps us understand how different models build their images over time and reveals key differences between fast and slow models.</figcaption>
</figure>

<h3>Using DT-Visualization to Debug Generation Issues</h3>
<p>
This visualization technique proves valuable as a debugging tool. For example, when we asked a model to generate "an image of a dog and cat sitting on a sofa," the final image contained only a dog. Using DT-Visualization, we could see that the model had actually started to create a cat face around the middle of the process, but later changed its mind and removed it.
</p>
<p>
This insight helps explain why diffusion models sometimes "forget" to include elements mentioned in the prompt—they begin creating certain features but abandon them during later refinement steps.
</p>

<figure class="center_image" style="margin-top: 30px">
  <center><img src="images/paper/debug.png" style="width:100%; max-width:800px"></center>
  <figcaption>Figure 4: DT-Visualization reveals how models can change course during generation. Here, when prompted to create "an image of dog and cat sitting on sofa," the model initially included a cat face (visible at timestep T=10) but removed it from the final output.</figcaption>
</figure>

<h3>The Key to Diversity: Early Generation Steps</h3>
<p>
When we applied DT-Visualization to compare base and distilled models, we discovered something crucial: the initial diffusion steps disproportionately determine the overall structure and diversity of the image, while later steps mainly refine details.
</p>
<p>
Distilled models appear to commit to a final image structure almost immediately after the first denoising step, while base models explore and develop structural elements across multiple steps. This early commitment explains the lack of diversity—distilled models essentially "make up their mind" too quickly about what the image will be.
</p>

<figure class="center_image" style="margin-top: 30px">
  <center><img src="images/paper/dt_viz.png" style="width:100%; max-width:800px"></center>
  <figcaption>Figure 5: Standard visualizations (left) show minimal differences between model types, but our DT-Visualization technique (right) reveals that distilled models commit to final image structure almost immediately, while base models gradually refine structure across multiple steps.</figcaption>
</figure>

<h2>Our Solution: Diversity Distillation</h2>
<p>
Based on our findings, we developed a hybrid approach called "Diversity Distillation." This method uses the base model for just the first critical timestep—where the most important structural decisions are made—and then switches to the faster distilled model for all remaining steps.
</p>
<p>
By letting the slow model handle the crucial creative exploration phase and the fast model handle the detail refinement, we get the best of both worlds: the diversity of base models with nearly the speed of distilled ones.
</p>

<figure class="center_image" style="margin-top: 30px">
  <center><img src="images/paper/diverse.png" style="width:100%; max-width:800px"></center>
  <figcaption>Figure 6: Visual comparison of generation diversity. Each row shows different outputs from the same prompt using different random seeds. The base model (left) produces diverse results but is slow. The distilled model (middle) is fast but produces similar images. Our hybrid approach (right) maintains diversity while keeping the speed advantage.</figcaption>
</figure>

<h2>Remarkable Results</h2>
<p>
Our experiments revealed something unexpected: this hybrid approach not only restores the diversity lost during distillation but actually exceeds the diversity of the original base model. Meanwhile, it maintains nearly the same computational efficiency as fully distilled inference, creating images in just 0.64 seconds compared to the base model's 9.22 seconds.
</p>
<p>
We confirmed these results using both objective metrics like FID scores (which measure how well the generated images match real-world distributions) and by examining sample diversity within the same prompt (how different the outputs look when using different random seeds).
</p>

<figure class="center_image" style="margin-top: 30px">
  <center><img src="images/paper/table.png" style="width:100%; max-width:500px"></center>
  <figcaption>Figure 7: Our method achieves better FID scores (lower is better) than both the base and distilled models while maintaining similar speed to distilled models. This means our approach creates both higher quality and more diverse images without sacrificing efficiency.</figcaption>
</figure>

<figure class="center_image" style="margin-top: 30px">
  <center><img src="images/paper/hyperparam.png" style="width:100%; max-width:800px"></center>
  <figcaption>Figure 8: We examined various parameters to optimize our method, including guidance scale, number of timesteps, and computational trade-offs. Even using the base model for just the first step provides substantial diversity benefits.</figcaption>
</figure>

<h2>Practical Implications</h2>
<p>
This research challenges the traditional assumption that there must be a trade-off between speed and diversity in diffusion models. By understanding exactly where in the process diversity is determined, we can design hybrid approaches that maintain creative variety without sacrificing computational efficiency.
</p>
<p>
For practitioners, our findings mean that existing investments in control mechanisms like Concept Sliders, LoRAs, and custom fine-tunings can be seamlessly transferred between model types. This "control distillation" enables precise creative direction even in the fastest models.
</p>
<p>
Our DT-Visualization technique provides a valuable new tool for researchers and developers to debug and understand diffusion models, potentially leading to further innovations in model architecture and training.
</p>

<h2>How to cite</h2>

<p>The paper can be cited as follows.</p>

<div class="card">
<h3 class="card-header">bibliography</h3>
<div class="card-block">
<p style="text-indent: -3em; margin-left: 3em;" class="card-text clickselect">
Rohit Gandikota, David Bau. "<em>Distilling Diversity and Control in Diffusion Models.</em>"
arXiv preprint arXiv:2503.xxxxx (2025).</nobr>
</p>
</div>
<h3 class="card-header">bibtex</h3>
<div class="card-block">
<pre class="card-text clickselect">
@article{gandikota2025distilling,
  title={Distilling Diversity and Control in Diffusion Models},
  author={Rohit Gandikota and David Bau},
  journal={arXiv preprint arXiv:2503.xxxxx}
  year={2025}
}
</pre>
</div>
</div>
</p>

</div>
</div><!--row -->
</div> <!-- container -->

<footer class="nd-pagefooter">
  <div class="row">
    <div class="col-6 col-md text-center">
      <a href="https://baulab.info/">About the Bau Lab</a>
    </div>
  </div>
</footer>

</body>
<script>
$(document).on('click', '.clickselect', function(ev) {
  var range = document.createRange();
  range.selectNodeContents(this);
  var sel = window.getSelection();
  sel.removeAllRanges();
  sel.addRange(range);
});
</script>
</html>